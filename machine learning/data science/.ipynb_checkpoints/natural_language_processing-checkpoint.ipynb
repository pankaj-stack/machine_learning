{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. It is the branch of machine learning which is about analyzing any text and handling predictive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk# Natural Language Toolkit(nltk)\n",
    "# The Natural Language Toolkit (nltk) is a platform used for building Python programs \n",
    "# that work with human language data for applying in statistical natural language \n",
    "# processing (NLP). It contains text processing libraries for tokenization, parsing,\n",
    "# classification, stemming, tagging and semantic reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.listdir(r'C:\\Users\\Atul_kumar_prajapati\\Documents\\DS\\NLP\\Dataset\\C50\\C50train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A break-in at the U.S. Justice Department\\'s World Wide Web site last week highlighted the Internet\\'s continued vulnerability to hackers.\\nUnidentified hackers gained access to the department\\'s web page on August 16 and replaced it with a hate-filled diatribe labelled the \"Department of Injustice\" that included a swastika and a picture of Adolf Hitler.\\nJustice officials quickly pulled the plug on the vandalised page, but the security flaws that allowed hackers to gain entry likely exist in thousands of other corporate and government web sites, security experts said.\\n\"The vast majority of sites are vulnerable,\" said Richard Power, senior analyst at the Computer Security Institute. \"The Justice Department shouldn\\'t be singled out.\"\\nJustice Department officials said the compromised web site was not connected to any computers containing sensitive files. The web site (http://www.usdoj.gov) included copies of press releases, speeches and other publicly available information.\\nThe security breach \"is just like graffiti on the outside of the building,\" spokesman Bert Brandenburg said.\\nOther organisations have been targeted in the past. Last year, the Nation of Islam\\'s Million Man March web site was vandalised. And hackers make 250,000 attempts annually to break into U.S. military computers, according to a General Accounting Office report.\\nWindows Magazine recently found security flaws at web sites of a dozen major corporations. \"The web is spectacularly insecure,\" editor Mike Elgan said. Relying on security holes that had been documented by software manufacturers months earlier, the magazine\\'s specialists were able to gain various degrees of unauthorised access at the different sites.\\nElgan said hackers who are exploiting some of the same flaws are motivated by anger over the growth and commercialization of the Internet. \"A common theme is that hackers are fed up with non-hackers on the Internet,\" he said.\\nThe battle is not completely hopeless. \"You can secure a web site,\" Richard Power said. \"There\\'s all kinds of measures you can take. Most corporations and institutions don\\'t take them simply because nothing bad has happened to them yet.\"\\nSome sites are using multiple layers of security, well beyond simple password protection, to keep hackers out.\\nOne site mentioned by Windows Magazine was Fidelity Investments. Fidelity\\'s site advertises its mutual funds and disseminates information about personal finance but does not contain confidential customer information.\\nFidelity officials immediately closed the loophole identified by the magazine, a spokeswoman said. But multiple security measures previously in place would have prevented a security breach despite the hole, the spokeswoman added.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('/home/pankaj/Downloads/Dataset/C50/C50train/AaronPressman/2537newsML.txt')\n",
    "text=f.read()\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation(विभाजन)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A break-in at the U.S. Justice Department's World Wide Web site last week highlighted the Internet's continued vulnerability to hackers.\n",
      "\n",
      "Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the \"Department of Injustice\" that included a swastika and a picture of Adolf Hitler.\n",
      "\n",
      "Justice officials quickly pulled the plug on the vandalised page, but the security flaws that allowed hackers to gain entry likely exist in thousands of other corporate and government web sites, security experts said.\n",
      "\n",
      "\"The vast majority of sites are vulnerable,\" said Richard Power, senior analyst at the Computer Security Institute. \"The Justice Department shouldn't be singled out.\"\n",
      "\n",
      "Justice Department officials said the compromised web site was not connected to any computers containing sensitive files. The web site (http://www.usdoj.gov) included copies of press releases, speeches and other publicly available information.\n",
      "\n",
      "The security breach \"is just like graffiti on the outside of the building,\" spokesman Bert Brandenburg said.\n",
      "\n",
      "Other organisations have been targeted in the past. Last year, the Nation of Islam's Million Man March web site was vandalised. And hackers make 250,000 attempts annually to break into U.S. military computers, according to a General Accounting Office report.\n",
      "\n",
      "Windows Magazine recently found security flaws at web sites of a dozen major corporations. \"The web is spectacularly insecure,\" editor Mike Elgan said. Relying on security holes that had been documented by software manufacturers months earlier, the magazine's specialists were able to gain various degrees of unauthorised access at the different sites.\n",
      "\n",
      "Elgan said hackers who are exploiting some of the same flaws are motivated by anger over the growth and commercialization of the Internet. \"A common theme is that hackers are fed up with non-hackers on the Internet,\" he said.\n",
      "\n",
      "The battle is not completely hopeless. \"You can secure a web site,\" Richard Power said. \"There's all kinds of measures you can take. Most corporations and institutions don't take them simply because nothing bad has happened to them yet.\"\n",
      "\n",
      "Some sites are using multiple layers of security, well beyond simple password protection, to keep hackers out.\n",
      "\n",
      "One site mentioned by Windows Magazine was Fidelity Investments. Fidelity's site advertises its mutual funds and disseminates information about personal finance but does not contain confidential customer information.\n",
      "\n",
      "Fidelity officials immediately closed the loophole identified by the magazine, a spokeswoman said. But multiple security measures previously in place would have prevented a security breach despite the hole, the spokeswoman added.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent=text.split('\\n')#here regular expression is used for splitting the senteces from\n",
    "for i in sent:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens. Python breaks each logical line into a sequence of elementary lexical components known as tokens. Each token corresponds to a substring of the logical line. The normal token types are identifiers, keywords, operators, delimiters, and literals, as covered in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A break-in at the U.S. Justice Department's World Wide Web site last week highlighted the Internet's continued vulnerability to hackers.\n",
      "\n",
      "Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the \"Department of Injustice\" that included a swastika and a picture of Adolf Hitler.\n",
      "\n",
      "Justice officials quickly pulled the plug on the vandalised page, but the security flaws that allowed hackers to gain entry likely exist in thousands of other corporate and government web sites, security experts said.\n",
      "\n",
      "\"The vast majority of sites are vulnerable,\" said Richard Power, senior analyst at the Computer Security Institute.\n",
      "\n",
      "\"The Justice Department shouldn't be singled out.\"\n",
      "\n",
      "Justice Department officials said the compromised web site was not connected to any computers containing sensitive files.\n",
      "\n",
      "The web site (http://www.usdoj.gov) included copies of press releases, speeches and other publicly available information.\n",
      "\n",
      "The security breach \"is just like graffiti on the outside of the building,\" spokesman Bert Brandenburg said.\n",
      "\n",
      "Other organisations have been targeted in the past.\n",
      "\n",
      "Last year, the Nation of Islam's Million Man March web site was vandalised.\n",
      "\n",
      "And hackers make 250,000 attempts annually to break into U.S. military computers, according to a General Accounting Office report.\n",
      "\n",
      "Windows Magazine recently found security flaws at web sites of a dozen major corporations.\n",
      "\n",
      "\"The web is spectacularly insecure,\" editor Mike Elgan said.\n",
      "\n",
      "Relying on security holes that had been documented by software manufacturers months earlier, the magazine's specialists were able to gain various degrees of unauthorised access at the different sites.\n",
      "\n",
      "Elgan said hackers who are exploiting some of the same flaws are motivated by anger over the growth and commercialization of the Internet.\n",
      "\n",
      "\"A common theme is that hackers are fed up with non-hackers on the Internet,\" he said.\n",
      "\n",
      "The battle is not completely hopeless.\n",
      "\n",
      "\"You can secure a web site,\" Richard Power said.\n",
      "\n",
      "\"There's all kinds of measures you can take.\n",
      "\n",
      "Most corporations and institutions don't take them simply because nothing bad has happened to them yet.\"\n",
      "\n",
      "Some sites are using multiple layers of security, well beyond simple password protection, to keep hackers out.\n",
      "\n",
      "One site mentioned by Windows Magazine was Fidelity Investments.\n",
      "\n",
      "Fidelity's site advertises its mutual funds and disseminates information about personal finance but does not contain confidential customer information.\n",
      "\n",
      "Fidelity officials immediately closed the loophole identified by the magazine, a spokeswoman said.\n",
      "\n",
      "But multiple security measures previously in place would have prevented a security breach despite the hole, the spokeswoman added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents=nltk.sent_tokenize(text)\n",
    "for i in sents:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg1=\"Hello Mr. John,how're you? I'm fine. But python is great!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Mr. John,how're you?\", \"I'm fine.\", 'But python is great!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize# It's a sentence tokenizer\n",
    "sent_tok=sent_tokenize(eg1)\n",
    "sent_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Mr. John,how're you?\", \"I'm fine.\", 'But python is great!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence=sent_tokenize(eg1)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " \"John,how're\",\n",
       " 'you?',\n",
       " \"I'm\",\n",
       " 'fine.',\n",
       " 'But',\n",
       " 'python',\n",
       " 'is',\n",
       " 'great!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tok=WhitespaceTokenizer()\n",
    "print(tok)\n",
    "word_tok2=tok.tokenize(eg1)\n",
    "word_tok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.treebank.TreebankWordTokenizer object at 0x7f84dd5364d0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'John',\n",
       " ',',\n",
       " 'how',\n",
       " \"'re\",\n",
       " 'you',\n",
       " '?',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'fine.',\n",
       " 'But',\n",
       " 'python',\n",
       " 'is',\n",
       " 'great',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tok=TreebankWordTokenizer()\n",
    "print(tok)\n",
    "word_tok3=tok.tokenize(eg1)\n",
    "word_tok3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'John',\n",
       " ',',\n",
       " 'how',\n",
       " \"'\",\n",
       " 're',\n",
       " 'you',\n",
       " '?',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'fine',\n",
       " '.',\n",
       " 'But',\n",
       " 'python',\n",
       " 'is',\n",
       " 'great',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token=WordPunctTokenizer()\n",
    "print(token)\n",
    "sentence=token.tokenize(eg1)\n",
    "sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'sir,', 'how', 'are', 'you', '?']\n",
      "['hello', 'sir', ',', 'how', 'are', 'you', '?'] 7\n",
      "['hello sir, how are you ?']\n",
      "['hello', 'sir', ',', 'how', 'are', 'you', '?']\n",
      "['hello', 'sir', ',', 'how', 'are', 'you', '?']\n",
      "RegexpTokenizer(pattern='hello sir, how are you ?', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "f=open('/home/pankaj/Downloads/Dataset/C50/C50train/AaronPressman/2537newsML.txt')\n",
    "data=f.read()\n",
    "# data\n",
    "# for i in data.split('\\n'):\n",
    "#     print(i)\n",
    "#     print()\n",
    "from nltk import sent_tokenize\n",
    "# sentences=sent_tokenize(data)\n",
    "# for i in sentences:\n",
    "#     print(i)#Here i is a token\n",
    "#     print()\n",
    "from nltk import word_tokenize\n",
    "# word=word_tokenize(data)\n",
    "# for i in word:\n",
    "#     print(i)\n",
    "#     print()\n",
    "# eg1=\"Hello Mr. John,how're you? I'm fine. But python is great!\"\n",
    "# s=sent_tokenize(eg1)\n",
    "# for sentence in s:\n",
    "#     print(sentence)\n",
    "# print('###############################')\n",
    "# for j in word_tokenize(eg1):\n",
    "#     print(j)\n",
    "# print(word_tokenize(eg1))\n",
    "from nltk import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer#It is similar to word_tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "ws=WhitespaceTokenizer()\n",
    "print(ws.tokenize('hello sir, how are you ?'))\n",
    "wt=word_tokenize('hello sir, how are you ?')\n",
    "print(wt,len(wt))\n",
    "st=sent_tokenize('hello sir, how are you ?')\n",
    "print(st)\n",
    "tbt=TreebankWordTokenizer()\n",
    "print(tbt.tokenize('hello sir, how are you ?'))\n",
    "wpt=WordPunctTokenizer()\n",
    "print(wpt.tokenize('hello sir, how are you ?'))\n",
    "ret=RegexpTokenizer('hello sir, how are you ?')\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTokenizer(pattern=\"Hello Mr. John,how're you? I'm fine. But python is great!\", gaps=True, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n"
     ]
    }
   ],
   "source": [
    "#RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token=RegexpTokenizer(eg1,gaps=True)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wasn', 'o', 'd', 'which', 'about', 'he', 'hasn', 'shan', 'the', \"aren't\", 'whom', 'that', 'at', 'don', 'me', 'ours', 'yourselves', \"don't\", 've', 'being', 't', 'how', 'a', 'does', 'having', 'her', 'ourselves', \"isn't\", 'until', 'they', 'under', 'is', 'doesn', \"needn't\", 'itself', 'further', 'couldn', 'this', 'his', 'didn', 'mightn', \"didn't\", \"weren't\", 'aren', 'haven', 'from', 'in', \"you've\", 'any', 'same', 'hadn', 'between', \"mightn't\", 'shouldn', 'its', 'before', 'theirs', 'are', 'be', 'for', 'too', \"it's\", 'up', 'to', 'if', 'here', 'their', 'your', 'do', 'myself', 'no', 'such', \"couldn't\", 'yourself', \"that'll\", 'again', 'each', 'mustn', 'not', 'or', 'an', 'than', \"shan't\", 'then', 'after', 'yours', \"hasn't\", \"wasn't\", 'was', 'below', 'm', 'will', 'll', 'because', 'most', 'now', 'have', \"wouldn't\", 'doing', 'hers', 'into', 're', 'himself', 'out', 'just', 'what', 'had', 'has', 'did', 'am', 'over', 'nor', 'were', 'why', \"you'll\", 'off', 'isn', \"shouldn't\", 'against', 'few', 'on', 'by', 'so', 'should', 'through', 'above', 'ma', 'won', 'very', 'y', 'themselves', 'with', 'only', 'when', 'where', \"hadn't\", 'during', 'more', \"haven't\", \"mustn't\", \"you're\", 'own', 'there', 'all', 'those', 'she', 'him', 's', 'ain', 'my', 'been', 'who', 'but', 'i', \"you'd\", \"should've\", 'needn', 'other', 'them', \"she's\", 'as', 'once', 'weren', 'and', 'both', 'while', \"won't\", 'wouldn', 'of', 'we', 'it', 'you', 'these', \"doesn't\", 'down', 'some', 'herself', 'can', 'our'}\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "#How to avoid Stop words from the given data\n",
    "from nltk.corpus import stopwords #corpus :-> मानवशरीर,देह,काय\n",
    "stop_words=set(stopwords.words('english'))#english,dutch,french,korean,...\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', '.', 'John', ',', \"'\", '?', 'I', \"'\", 'fine', '.', 'But', 'python', 'great', '!']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#List comprehension in python\n",
    "filtered_arr=[w for w in sentence if w not in stop_words]#append kya karna hai---> w\n",
    "print(filtered_arr)\n",
    "print(len(filtered_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29th,January 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "Hello Mr John how re you I m fine But python is great "
     ]
    }
   ],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "punctuation=string.punctuation\n",
    "print(punctuation,end=' ')\n",
    "print()\n",
    "for i in sentence:\n",
    "    if i not in punctuation or i in stopwords.words('english'):\n",
    "        print(i,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total punctuation is : 32\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation=string.punctuation\n",
    "print(f'total punctuation is : {len(punctuation)}\\n{punctuation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', '.', 'John', ',', 'how', \"'\", 're', 'you', '?', 'I', \"'\", 'm', 'fine', '.', 'But', 'python', 'is', 'great', '!']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', '.', 'John', ',', 'how', \"'\", 're', 'you', '?', 'I', \"'\", 'm', 'fine', '.', 'But', 'python', 'is', 'great', '!']\n",
      "\n",
      "['Hello', 'Mr', 'John', 'how', 're', 'you', 'I', 'm', 'fine', 'But', 'python', 'is', 'great']\n"
     ]
    }
   ],
   "source": [
    "filtered_punc=[]\n",
    "print(sentence)\n",
    "print()\n",
    "for w in sentence:\n",
    "    if w not in string.punctuation:\n",
    "        filtered_punc.append(w)\n",
    "print(filtered_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs ---> run\n",
      "ran ---> ran\n",
      "play ---> play\n",
      "playing ---> play\n",
      "played ---> play\n",
      "studies ---> studi\n",
      "goes ---> goe\n"
     ]
    }
   ],
   "source": [
    "# stemming/lemmatization--->They convert the word into the root word.\n",
    "# It chope-off  the ---> s,es,in,ed etc...\n",
    "#PorterStemmer/LancasterStemmer/SnowballStemmer etc...\n",
    "words=['runs','ran','play','playing','played','studies','goes']\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stem=PorterStemmer()\n",
    "for i in words:\n",
    "    print(i,'--->',p_stem.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs ---> run\n",
      "ran ---> ran\n",
      "play ---> play\n",
      "playing ---> play\n",
      "played ---> play\n",
      "studies ---> studi\n",
      "goes ---> goe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer#it's better \n",
    "l_stem=LancasterStemmer()\n",
    "for i in words:\n",
    "        print(i,'--->',p_stem.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs ---> run\n",
      "ran ---> ran\n",
      "play ---> play\n",
      "playing ---> play\n",
      "played ---> play\n",
      "studies ---> studi\n",
      "goes ---> goe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stem=SnowballStemmer('english')#<-----------------------english-----------\n",
    "for i in words:\n",
    "        print(i,'--->',s_stem.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "#It is the process of converting a word to its base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs --> run\n",
      "ran --> run\n",
      "play --> play\n",
      "playing --> play\n",
      "played --> play\n",
      "studies --> study\n",
      "goes --> go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(i,'-->',lemma.lemmatize(i,pos='v'))# pos=part of speech\n",
    "                                             # n=noun,v=verb,a=adjective,r=adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs--->run\n",
      "ran--->ran\n",
      "play--->play\n",
      "playing--->playing\n",
      "played--->played\n",
      "studies--->study\n",
      "goes--->go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(f'{i}--->{wordnet_lemmatizer.lemmatize(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feet', 'NNS')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to automate the above task\n",
    "nltk.pos_tag(['feet'])#NNS is Noun Plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('Mr', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " ('how', 'WRB'),\n",
       " ('re', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('I', 'PRP'),\n",
       " ('m', 'VBP'),\n",
       " ('fine', 'JJ'),\n",
       " ('But', 'CC'),\n",
       " ('python', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('great', 'JJ')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(filtered_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNS')# To find the tag information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['tag'])[0][1][0]# to find the 1st word of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr\n",
      "John\n",
      "how\n",
      "re\n",
      "you\n",
      "I\n",
      "m\n",
      "fine\n",
      "But\n",
      "python\n",
      "be\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "#Automation ok task\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    '''Map POS tag to first character lemmatize() accepts'''\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={'J':wordnet.ADJ,\n",
    "             'N':wordnet.NOUN,\n",
    "             'V':wordnet.VERB,\n",
    "             'R':wordnet.ADV\n",
    "             }\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "for w in filtered_punc:\n",
    "    print(lemma.lemmatize(w,get_wordnet_pos(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr\n",
      "John\n",
      "how\n",
      "re\n",
      "you\n",
      "I\n",
      "m\n",
      "fine\n",
      "But\n",
      "python\n",
      "be\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\n",
    "                'J':wordnet.ADJ,\n",
    "                'N':wordnet.NOUN,\n",
    "                'V':wordnet.VERB,\n",
    "                'R':wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "for i in filtered_punc:\n",
    "    print(lemma.lemmatize(i,get_wordnet_pos(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "ran\n",
      "play\n",
      "play\n",
      "played\n",
      "study\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(lemma.lemmatize(i,get_wordnet_pos(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is faster while lemmatization is slow but lemmatization is better than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
